---
title: "Classification: Supervised Learning"
subtitle: "Naive Bayes Classifier"
author: "Satish G. Iyengar"
format: html
number-sections: true
editor: visual
---

```{r, warning=FALSE, message=FALSE, error=FALSE, echo=FALSE}
library(naivebayes)
library(tidyverse)
library(ggplot2)
library(psych)

```

## Problem Definition

-   Build a model to predict if student $A$ will secure an admit for graduate studies.

### Data

```{r, echo=TRUE, warning=FALSE, message=FALSE, error=FALSE}
dat <- readr::read_csv('binary.csv')
nrow(dat)
```

```{r}
dat %>% head() %>% knitr::kable()
```

Variables admit and rank should not be interpreted as numerical values. They are factors.

```{r}
dat$admit <- as.factor(dat$admit)
dat$rank <- as.factor(dat$rank)

```

#### Dependent variable/Target variable

-   'admit', $Y \in \{0, 1\}$

#### Independent variables/Predictors

-   gre $(X_1)$
-   gpa $(X_2)$
-   rank (of their undergrad school) $(X_3)$

#### ML Model

-   Building a machine learning model to predict $Y$ given predictors $X_1, X_2, X_3$, is to learn the function $f$ from the given labelled data where, $$\textcolor{darkblue} {\hat{Y} = f(X_1, X_2, X_3)}$$

## Data Visualization

### How correlated are the predictor variables?

```{r}
dat %>% select(-admit) %>% pairs.panels()

```

### Sensitivity of gre score (to admit)

```{r}
dat %>% 
  ggplot(., aes(x = admit, y = gre, fill = admit)) +
  geom_boxplot()

```

```{r}
dat %>% 
  ggplot(., aes(x = gre, fill = admit)) +
  geom_density(alpha = 0.5)

```

### Sensitivity of gpa score (to admit)

```{r}
dat %>% 
  ggplot(., aes(x = admit, y = gpa, fill = admit)) +
  geom_boxplot()

```

```{r}
dat %>% 
  ggplot(., aes(x = gpa, fill = admit)) +
  geom_density(alpha = 0.5)

```

### Admit vs. Rank

```{r}
tab1 <- xtabs(data = dat, ~admit + rank)
tab1
```

## Building an ML model

-   Data partitioning
    -   Training set: Sample of data used to fit the model
    -   Validation set: Sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters.
    -   Test set: Sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.

### Data partitioning

```{r}
set.seed(1234)
ind <- sample(2, nrow(dat), replace = T, prob = c(0.8, 0.2))
train <- dat[ind==1,]
test <- dat[ind==2, ]
nrow(train)
nrow(test)
```

### Naive Bayes Classifier

-   Bayes Rule

$$ P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$

-   For example,

$$ P(Admit = 1| Rank = 1) = \frac{P(Rank = 1| Admit = 1) \cdot P(Admit = 1)}{P(Rank = 1)}$$

#### What is naive about the Navie Bayes Classifier?

In general,

$$ P(Y = C_k | X = x)  = \frac{\overbrace{P(Y = C_k)}^{prior} \cdot \overbrace{P(X = x | Y = C_k)}^{Likelihood}}{\underbrace{P(X = x)}_{\text{independent of class label}} }$$.

If $X = \{X_1, X_2, \ldots, X_d\}$, the naive Bayes classifier makes an indpendence assumption,

$$ P(X_1, X_2, \ldots, X_d | Y = C_k) = \prod_{i=1}^d P(X_i = x_i | Y = C_k)$$.

That is, given the class label all predictors are *naively* assumed to be statistically independent. Therefore,

$$ P(Y = C_k | X = x) \propto P(Y = C_k) \cdot \prod_{i=1}^d P(X_i = x_i | Y = C_k) $$

#### Applications

-   Spam filter: Classify emails as spam/non-spam

-   Recommender systems

#### Function call in R to build a Naive-Bayes Classifier

```{r}
model <- naive_bayes(admit ~ ., data = train)
model
```

-   **A priori probabilities:** In the training set, 68.6 % of the data belongs to admit category '0' or and 31.3% of the students belong the admit category.

-   **Quantitative data**: Likelihood models for GPA and GRE are assumed to be normal distributions with estimated mean and standard deviations.

```{r}
model$tables$gre
model$tables$gpa
```

-   **Categorical data**: For categorical data, the model computes the likelihhod of each level. Therefore, we have,

    -   $P(Rank = 1 | Admit = 0) = `r model$tables$rank[1,1] `$,
    -   $P(Rank = 1 | Admit = 1) = `r model$tables$rank[1,2]`$,
    -   ...

```{r}
model$tables$rank
```

### Performance Assessment

#### Goodness-of-fit

-   How well have I learnt from the training set?

```{r, warning=FALSE, message=FALSE}
p <- predict(model, train, type = "prob")
posterior_train <- data.frame('posterior_no_admit' = p[,1],
                         'posterior_admit' = p[,2]) %>%
  bind_cols(train)
head(posterior_train)

```

```{r}
posterior_train %>% 
  ggplot(., aes(x = admit, y=posterior_admit, fill = admit)) + 
  geom_boxplot()

```

#### Confusion matrix

```{r}
#Confusion matrix for train data
p1 <- predict(model, train)
posterior_train <- posterior_train %>%
  mutate(
    posterior_class = p1
  )

tab1 <- table(posterior_train$admit, 
              posterior_train$posterior_class)
tab1 %>% knitr::kable()
```

-   **Misclassification Rate**: `r 1 - sum(diag(tab1))/sum(tab1)`

#### Confusion matrix: Test Set

```{r}
p <- predict(model, test, type = "prob")
posterior_test <- data.frame('posterior_no_admit' = p[,1],
                         'posterior_admit' = p[,2]) %>%
  bind_cols(test)
head(posterior_test)

#Confusion matrix for test data
p1 <- predict(model, test)
posterior_test <- posterior_test %>%
  mutate(
    posterior_class = p1
  )

tab1 <- table(posterior_test$admit, 
              posterior_test$posterior_class)
tab1 %>% knitr::kable()
```

-   **Misclassification Rate**: `r 1 - sum(diag(tab1))/sum(tab1)`
